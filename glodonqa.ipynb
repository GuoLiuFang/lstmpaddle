{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tarfile\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "__all__ = ['build_dict', 'train', 'test', 'convert']\n",
    "def word_dict(cutoff=150):\n",
    "    \"\"\"\n",
    "    从语料中建立一个. Build a word dictionary from the corpus.\n",
    "    :return: Word dictionary\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "    return build_dict(re.compile(\"glodon/((train)|(test))/key./.*?/.*\\.txt$\"), cutoff)\n",
    "def build_dict(pattern, cutoff):\n",
    "    \"\"\"\n",
    "    Build a word dictionary from the corpus. Keys of \n",
    "    the dictionary are words, and values are zero-based\n",
    "    IDs of these words.\n",
    "    \"\"\"\n",
    "    word_freq = collections.defaultdict(int)\n",
    "    for doc, _ in tokenize(pattern):\n",
    "        for word in doc:\n",
    "            word_freq[word] += 1\n",
    "    # not sure if we should prune less-frequent words here.\n",
    "    word_freq = filter(lambda x: x[1] > cutoff, word_freq.items())\n",
    "    \n",
    "    dictionary = sorted(word_freq, key=lambda x: (-x[1], x[0]))\n",
    "    words, _ = list(zip(*dictionary))\n",
    "    word_idx = dict(zip(words, xrange(len(words))))\n",
    "    word_idx['<unk>'] = len(words)\n",
    "    return word_idx\n",
    "def tokenize(pattern):\n",
    "    \"\"\"\n",
    "    Read files that match the given pattern.\n",
    "    Tokenize and yield each file.\n",
    "    :return:(list,label)\n",
    "    \"\"\"\n",
    "    labelpattern = re.compile(\"glodon/((train)|(test))/key./(?P<label>.*?)/.*\\.txt$\")\n",
    "    with tarfile.open('glodon.tar.gz') as tarf:\n",
    "        tf = tarf.next()\n",
    "        while tf != None:\n",
    "            if bool(pattern.match(tf.name)):\n",
    "                # newline and punctuations removal and ad-hoc tokenization.\n",
    "#                 print(\"filename is %s\" % tf.name)\n",
    "                match = labelpattern.match(tf.name)\n",
    "                label_idx = match.group(\"label\")\n",
    "#                 print('<>'.join(jieba.cut(tarf.extractfile(tf).read().rstrip(\"\\n\\r\").translate(None, string.punctuation).lower())))\n",
    "                yield (jieba.cut(tarf.extractfile(tf).read().rstrip(\"\\n\\r\").translate(None, string.punctuation).lower()), label_idx)\n",
    "            tf = tarf.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = word_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in sorted(x, key=x.get, reverse=True):\n",
    "    print(\"key---%s----value--%s\" % (y,x[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(word_idx, level=3):\n",
    "    \"\"\"\n",
    "    training set creator..\n",
    "    It returns a reader creator , each sample\n",
    "    in the reader is an zero-based ID sequence \n",
    "    and label in [0,1]\n",
    "    \"\"\"\n",
    "    return reader_creator(re.compile(\"glodon/train/key%d/.*?/.*\\.txt$\" % level), word_idx)\n",
    "def reader_creator(pattern, word_idx):\n",
    "    UNK = word_idx['<unk>']\n",
    "    INS = []\n",
    "    def load(pattern, out):\n",
    "        for doc, label in tokenize(pattern):\n",
    "            out.append(([word_idx.get(w, UNK) for w in doc], label))\n",
    "    load(pattern, INS)\n",
    "#     def reader():\n",
    "#         for doc, label in INS:\n",
    "#             yield doc, label\n",
    "#     return reader\n",
    "#         def reader():\n",
    "    for doc, label in INS:\n",
    "        yield doc, label\n",
    "#     return reader\n",
    "def test(word_idx, level=3):\n",
    "    return reader_creator(re.compile(\"glodon/test/key%d/.*?/.*\\.txt$\" % level), word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = train()\n",
    "for doc, label in train(x):\n",
    "    print(\"---%s----%s---\" % ('='.join([str(e) for e in doc]), str(label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
