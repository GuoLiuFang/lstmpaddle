{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练中文词向量\n",
    "\n",
    "这个demo展示Paddle如何训练中文的词向量模型。这里使用了处理过的维基百科中文语料作为训练语料。所有训练文件均分好词，放置在`wiki_data/data`目录中。\n",
    "\n",
    "首先我们先读取所有的文件，生成词表文件，并缓存到本地的目录中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating word dictionary in the first time.\n",
      "Saving to word_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "import cPickle\n",
    "import os\n",
    "import collections\n",
    "\n",
    "try:  # load word dict from disk\n",
    "    with open(\"word_dict.pkl\") as f:\n",
    "        word_dict = cPickle.load(f)\n",
    "except:  # generate word dict in the first time\n",
    "    print 'Generating word dictionary in the first time.'\n",
    "    word_dict = collections.defaultdict(int)\n",
    "    for dirpath, dirnames, filenames in os.walk(\"data/\"):\n",
    "        if len(filenames) != 0:\n",
    "            for fn in filenames:\n",
    "                with open(os.path.join(dirpath, fn)) as f:\n",
    "                    for line in f:\n",
    "                        for w in line.strip().split():\n",
    "                            word_dict[w] += 1\n",
    "                            \n",
    "    items = list(word_dict.items())\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    word_dict = dict()\n",
    "    for i in xrange(len(items)):\n",
    "        word_dict[items[i][0]] = i\n",
    "    \n",
    "    print 'Saving to word_dict.pkl'\n",
    "    with open(\"word_dict.pkl\", \"w\") as f:\n",
    "        cPickle.dump(word_dict, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步，我们开始读数据的过程。在读数据过程中，我们将词转换为词ID。由于数据量本身不大，所以我们将全部数据全部读入内存中即可。\n",
    "\n",
    "同时，我们丢弃低频词，从而加快训练过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting words to word ids in the first time\n",
      "Saving to all_data.pkl\n"
     ]
    }
   ],
   "source": [
    "WORD_LIMIT=2000   # 只训练2000个词汇\n",
    "WINDOW_SIZE=11    # 训练窗口大小为11\n",
    "EMB_SIZE=32       # 设定词向量宽度\n",
    "NUM_PASSES = 20   # 设定训练轮数\n",
    "\n",
    "START_ID = WORD_LIMIT  # 句子开始标志\n",
    "END_ID = START_ID + 1  # 句子结束标志\n",
    "\n",
    "try:\n",
    "    with open(\"all_data.pkl\") as f:\n",
    "        all_data = cPickle.load(f)\n",
    "except:\n",
    "    print 'Converting words to word ids in the first time'\n",
    "    all_data = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(\"data/\"):\n",
    "        for fn in filenames:\n",
    "            with open(os.path.join(dirpath, fn)) as f:\n",
    "                for line in f:\n",
    "                    line = [word_dict[w] for w in line.strip().split() if word_dict[w] < WORD_LIMIT]\n",
    "                    line = [START_ID] + line + [END_ID]\n",
    "                    if len(line) >= WINDOW_SIZE:\n",
    "                        all_data.append(line)\n",
    "    \n",
    "    print 'Saving to all_data.pkl'\n",
    "    with open(\"all_data.pkl\", 'w') as f:\n",
    "        cPickle.dump(all_data, f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步开始配置reader_creator。 reader_creator是Paddle的一个概念，用户通过自定义reader_creator定义Paddle的输入数据。reader_creator是一个函数，他返回一个reader函数，而reader函数是一个可以返回每一条数据的iterable的函数。简单示例如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def word_reader_creator():\n",
    "    def reader():\n",
    "        global all_data  # access all data below\n",
    "        random.shuffle(all_data)\n",
    "        for line in all_data:\n",
    "            for i in xrange(len(line) - WINDOW_SIZE + 1):\n",
    "                yield line[i:i+WINDOW_SIZE]  # yield word ids from 0 to WINDOW_SIZE\n",
    "    \n",
    "    return reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面开始配置神经网络，这里配置一个简单的CBOW网络, trainer_count是物理 cpu_cores 的个数。。一会儿 ubutu 安装。htop 看看资源消耗情况。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.v2 as paddle\n",
    "paddle.init(use_gpu=False)\n",
    "words = [paddle.layer.data(name=\"word_%d\"%i, type=paddle.data_type.integer_value(WORD_LIMIT + 2)) \n",
    "         for i in xrange(WINDOW_SIZE)]\n",
    "\n",
    "embs = []\n",
    "for w in words[:WINDOW_SIZE / 2] + words[-WINDOW_SIZE / 2 + 1:]:\n",
    "    embs.append(paddle.layer.embedding(input=w, size=EMB_SIZE, param_attr=\n",
    "                                       paddle.attr.Param(name='emb', sparse_update=True)))\n",
    "\n",
    "with paddle.layer.mixed(size=EMB_SIZE) as sum_emb:\n",
    "    for emb in embs:\n",
    "        sum_emb += paddle.layer.identity_projection(input=emb)\n",
    "\n",
    "label = words[WINDOW_SIZE / 2]\n",
    "\n",
    "cost = paddle.layer.hsigmoid(input=sum_emb, label=label, num_classes=WORD_LIMIT+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面构建训练的参数，优化器，和trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = paddle.parameters.create(cost)\n",
    "optimizer = paddle.optimizer.RMSProp(learning_rate=1e-3)\n",
    "trainer = paddle.trainer.SGD(cost, parameters, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步书写event_handler。Paddle的event handler是在训练过程中响应训练事件的回调函数，在这里用户可以对训练误差进行监控，保存模型等。\n",
    "\n",
    "进而开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".Pass 0, Batch 0, AvgCost 7.624405\n",
      "..............................Pass 0\n",
      ".Pass 1, Batch 0, AvgCost 7.305836\n",
      "..............................Pass 1\n",
      ".Pass 2, Batch 0, AvgCost 7.054216\n",
      "..............................Pass 2\n",
      ".Pass 3, Batch 0, AvgCost 6.944893\n",
      "..............................Pass 3\n",
      ".Pass 4, Batch 0, AvgCost 6.885181\n",
      "..............................Pass 4\n",
      ".Pass 5, Batch 0, AvgCost 6.844780\n",
      "..............................Pass 5\n",
      ".Pass 6, Batch 0, AvgCost 6.814500\n",
      "..............................Pass 6\n",
      ".Pass 7, Batch 0, AvgCost 6.787237\n",
      "..............................Pass 7\n",
      ".Pass 8, Batch 0, AvgCost 6.761542\n",
      "..............................Pass 8\n",
      ".Pass 9, Batch 0, AvgCost 6.737207\n",
      "..............................Pass 9\n",
      ".Pass 10, Batch 0, AvgCost 6.713035\n",
      "..............................Pass 10\n",
      ".Pass 11, Batch 0, AvgCost 6.689688\n",
      "..............................Pass 11\n",
      ".Pass 12, Batch 0, AvgCost 6.666462\n",
      "..............................Pass 12\n",
      ".Pass 13, Batch 0, AvgCost 6.643713\n",
      "..............................Pass 13\n",
      ".Pass 14, Batch 0, AvgCost 6.621316\n",
      "..............................Pass 14\n",
      ".Pass 15, Batch 0, AvgCost 6.599476\n",
      "..............................Pass 15\n",
      ".Pass 16, Batch 0, AvgCost 6.577802\n",
      "..............................Pass 16\n",
      ".Pass 17, Batch 0, AvgCost 6.556742\n",
      "..............................Pass 17\n",
      ".Pass 18, Batch 0, AvgCost 6.536019\n",
      "..............................Pass 18\n",
      ".Pass 19, Batch 0, AvgCost 6.515753\n",
      "..............................Pass 19\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p output\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "total_cost = 0.0\n",
    "counter = 0\n",
    "prefix=\"./output\"\n",
    "def event_handler(event):\n",
    "    global total_cost\n",
    "    global counter\n",
    "    if isinstance(event, paddle.event.EndIteration):\n",
    "        total_cost += event.cost\n",
    "        counter += 1\n",
    "        sys.stdout.write('.')\n",
    "        if event.batch_id % 100 == 0:\n",
    "            print \"Pass %d, Batch %d, AvgCost %f\" % (event.pass_id, event.batch_id, total_cost / counter)\n",
    "        if event.batch_id % 10000 == 0:\n",
    "            with gzip.open(os.path.join(prefix, \"model_%d_%d.tar.gz\" % (event.pass_id, event.batch_id)), 'w') as f:\n",
    "                parameters.to_tar(f)\n",
    "    if isinstance(event, paddle.event.EndPass):\n",
    "        print \"Pass %d\" % event.pass_id\n",
    "        with gzip.open(os.path.join(prefix, \"model_%d.tar.gz\" % event.pass_id), 'w') as f:\n",
    "            parameters.to_tar(f)\n",
    "\n",
    "trainer.train(paddle.batch(paddle.reader.buffered(word_reader_creator(), 16 * 4000), 3000),\n",
    "        num_passes=NUM_PASSES,\n",
    "        event_handler=event_handler,\n",
    "        feeding=[w.name for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，训练完20轮之后，所有的模型均保存在了output路径下，以备之后使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18956\r\n",
      "-rw-r--r-- 1 root root 482709 May 14 15:03 model_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 478947 May 14 15:03 model_0_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481346 May 14 15:03 model_1.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483481 May 14 15:04 model_10.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483489 May 14 15:04 model_10_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483718 May 14 15:04 model_11.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483617 May 14 15:04 model_11_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483693 May 14 15:04 model_12.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483760 May 14 15:04 model_12_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483955 May 14 15:05 model_13.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483827 May 14 15:05 model_13_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483917 May 14 15:05 model_14.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483837 May 14 15:05 model_14_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483978 May 14 15:05 model_15.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483899 May 14 15:05 model_15_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 484058 May 14 15:05 model_16.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483966 May 14 15:05 model_16_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 484067 May 14 15:05 model_17.tar.gz\r\n",
      "-rw-r--r-- 1 root root 484032 May 14 15:05 model_17_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 484113 May 14 15:05 model_18.tar.gz\r\n",
      "-rw-r--r-- 1 root root 484050 May 14 15:05 model_18_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 484161 May 14 15:05 model_19.tar.gz\r\n",
      "-rw-r--r-- 1 root root 484079 May 14 15:05 model_19_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 482705 May 14 15:03 model_1_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481023 May 14 15:03 model_2.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481407 May 14 15:03 model_2_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481135 May 14 15:04 model_3.tar.gz\r\n",
      "-rw-r--r-- 1 root root 480975 May 14 15:03 model_3_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481469 May 14 15:04 model_4.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481157 May 14 15:04 model_4_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481909 May 14 15:04 model_5.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481402 May 14 15:04 model_5_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 482449 May 14 15:04 model_6.tar.gz\r\n",
      "-rw-r--r-- 1 root root 481912 May 14 15:04 model_6_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 482793 May 14 15:04 model_7.tar.gz\r\n",
      "-rw-r--r-- 1 root root 482451 May 14 15:04 model_7_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483137 May 14 15:04 model_8.tar.gz\r\n",
      "-rw-r--r-- 1 root root 482889 May 14 15:04 model_8_0.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483300 May 14 15:04 model_9.tar.gz\r\n",
      "-rw-r--r-- 1 root root 483194 May 14 15:04 model_9_0.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把字典和 embeding 保存下来。。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数学的词向量为[-9.7721918e-03 -2.5700396e-01  6.3699834e-02  2.0338324e-01\n",
      " -1.4195548e-01 -1.4174160e-01  7.2824247e-03 -8.0354570e-05\n",
      " -2.9269570e-01 -4.3650303e-02  3.1950042e-02 -2.6742721e-01\n",
      " -4.4773249e-03  3.1809303e-01 -1.5773760e-01 -3.4221902e-02\n",
      " -1.5549351e-01 -1.0023664e-01  8.4953494e-02  4.0698282e-02\n",
      " -3.3146757e-01 -1.5535420e-01 -4.2131353e-02 -5.6917582e-02\n",
      " -1.3943800e-01  9.1001265e-02  5.5853993e-02 -1.1030980e-02\n",
      " -3.3745095e-01  5.5707268e-02 -3.9042065e-01 -2.4750319e-01]\n"
     ]
    }
   ],
   "source": [
    "embeddings = parameters.get(\"emb\").reshape(WORD_LIMIT + 2, EMB_SIZE)\n",
    "print(\"数学的词向量为%s\" % str(embeddings[word_dict['数学']]))\n",
    "import numpy\n",
    "numpy.save(\"emb\",embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
