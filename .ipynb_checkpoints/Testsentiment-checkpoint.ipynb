{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本分类涉及【文本表示】和【分类方法】两个问题。。在深度学习的方法出现之前。主流的文本表示为词袋模型 BOW..分类方法为 SVM 和 LR..\n",
    "对于，一段文本，BOW 表示会忽略其词的顺序，语法和句法。。。将这段文本仅仅看做是一个词的集合。。。因此，BOW 并不能充分表示文本的语义信息。\n",
    "深度学习模型，克服 BOW 表示的上述缺陷。。它在考虑词顺序的基础上，把文本映射到低纬度的语义空间。。。并且以端对端的方式进行文本表示及分类。。。\n",
    "\n",
    "\n",
    "对于，一般的短文本分类问题，简单的文本卷积网络。。《短文本处理的方法》\n",
    "\n",
    "\n",
    "循环神经网络是一种能对序列数据进行精确建模的有力工具。实际上，循环神经网络的理论计算能力是图灵完备的。。自然语言是一种典型的序列数据（词序列）。。目前效果最好的方法。。\n",
    "\n",
    "//首先，要进行的还是数据预处理。阶段的活儿。。\n",
    "使用双向循环网络。。\n",
    "bi-directional recurrent network,\n",
    "consisting three LSTM layers..\n",
    "input_dim: here is word dictionary dimension...\n",
    "emb_dim: dimension of word embedding...\n",
    "hid_dim: dimension of hidden layer..\n",
    "\n",
    "\n",
    "feeding用来指定，train_reader 和test_reader 返回的数据与模型配置中的 data_layer 的对应关系。。。现在，知道 word 的数据从哪里来了吧。。。。\n",
    "name='word'的数据，来自，Reader 返回的第0列数据。。。Reader 返回的第1列数据对应的是 name='label'的 data_layer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dictionary...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "[DEBUG 2018-05-24 15:19:57,129 __init__.py:111] Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "[DEBUG 2018-05-24 15:19:57,137 __init__.py:131] Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.457 seconds.\n",
      "[DEBUG 2018-05-24 15:19:57,592 __init__.py:163] Loading model cost 0.457 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "[DEBUG 2018-05-24 15:19:57,597 __init__.py:164] Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import paddle.v2 as paddle\n",
    "\n",
    "with_gpu = os.getenv('WITH_GPU', '0') != '0'\n",
    "\n",
    "\n",
    "def convolution_net(input_dim, class_dim=2, emb_dim=128, hid_dim=128):\n",
    "    data = paddle.layer.data(\"word\",\n",
    "                             paddle.data_type.integer_value_sequence(input_dim))\n",
    "    emb = paddle.layer.embedding(input=data, size=emb_dim)\n",
    "    conv_3 = paddle.networks.sequence_conv_pool(\n",
    "        input=emb, context_len=3, hidden_size=hid_dim)\n",
    "    conv_4 = paddle.networks.sequence_conv_pool(\n",
    "        input=emb, context_len=4, hidden_size=hid_dim)\n",
    "    output = paddle.layer.fc(\n",
    "        input=[conv_3, conv_4], size=class_dim, act=paddle.activation.Softmax())\n",
    "    lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(class_dim))\n",
    "    cost = paddle.layer.classification_cost(input=output, label=lbl)\n",
    "    return cost, output\n",
    "\n",
    "\n",
    "def stacked_lstm_net(input_dim,\n",
    "                     class_dim=2,\n",
    "                     emb_dim=128,\n",
    "                     hid_dim=512,\n",
    "                     stacked_num=3):\n",
    "    assert stacked_num % 2 == 1\n",
    "\n",
    "    fc_para_attr = paddle.attr.Param(learning_rate=1e-3)\n",
    "    lstm_para_attr = paddle.attr.Param(initial_std=0., learning_rate=1.)\n",
    "    para_attr = [fc_para_attr, lstm_para_attr]\n",
    "    bias_attr = paddle.attr.Param(initial_std=0., l2_rate=0.)\n",
    "    relu = paddle.activation.Relu()\n",
    "    linear = paddle.activation.Linear()\n",
    "\n",
    "    data = paddle.layer.data(\"word\",\n",
    "                             paddle.data_type.integer_value_sequence(input_dim))\n",
    "    emb = paddle.layer.embedding(input=data, size=emb_dim)\n",
    "\n",
    "    fc1 = paddle.layer.fc(\n",
    "        input=emb, size=hid_dim, act=linear, bias_attr=bias_attr)\n",
    "    lstm1 = paddle.layer.lstmemory(input=fc1, act=relu, bias_attr=bias_attr)\n",
    "\n",
    "    inputs = [fc1, lstm1]\n",
    "    for i in range(2, stacked_num + 1):\n",
    "        fc = paddle.layer.fc(\n",
    "            input=inputs,\n",
    "            size=hid_dim,\n",
    "            act=linear,\n",
    "            param_attr=para_attr,\n",
    "            bias_attr=bias_attr)\n",
    "        lstm = paddle.layer.lstmemory(\n",
    "            input=fc, reverse=(i % 2) == 0, act=relu, bias_attr=bias_attr)\n",
    "        inputs = [fc, lstm]\n",
    "\n",
    "    fc_last = paddle.layer.pooling(\n",
    "        input=inputs[0], pooling_type=paddle.pooling.Max())\n",
    "    lstm_last = paddle.layer.pooling(\n",
    "        input=inputs[1], pooling_type=paddle.pooling.Max())\n",
    "    output = paddle.layer.fc(\n",
    "        input=[fc_last, lstm_last],\n",
    "        size=class_dim,\n",
    "        act=paddle.activation.Softmax(),\n",
    "        bias_attr=bias_attr,\n",
    "        param_attr=para_attr)\n",
    "\n",
    "    lbl = paddle.layer.data(\"label\", paddle.data_type.integer_value(class_dim))\n",
    "    cost = paddle.layer.classification_cost(input=output, label=lbl)\n",
    "    return cost, output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # init\n",
    "    paddle.init(use_gpu=with_gpu)\n",
    "\n",
    "    #data\n",
    "    print 'load dictionary...'\n",
    "    import glodonqa\n",
    "    word_dict = glodonqa.word_dict()\n",
    "#     word_dict = paddle.dataset.imdb.word_dict()\n",
    "    dict_dim = len(word_dict)\n",
    "    import cPickle\n",
    "    with open(\"key3_dict.pkl\") as f:\n",
    "        key3_label_dict = cPickle.load(f)\n",
    "    class_dim = len(key3_label_dict)\n",
    "#     class_dim = 2\n",
    "    train_reader = paddle.batch(\n",
    "        paddle.reader.shuffle(\n",
    "            glodonqa.train(word_dict), buf_size=1000),\n",
    "#         paddle.dataset.imdb.train(word_dict), buf_size=1000),\n",
    "        batch_size=100)\n",
    "    test_reader = paddle.batch(\n",
    "        glodonqa.test(word_dict), batch_size=100)\n",
    "#             paddle.dataset.imdb.test(word_dict), batch_size=100)\n",
    "\n",
    "    feeding = {'word': 0, 'label': 1}\n",
    "\n",
    "\n",
    "#     [cost, output] = convolution_net(dict_dim, class_dim=class_dim)\n",
    "    [cost, output] = stacked_lstm_net(dict_dim, class_dim=class_dim, stacked_num=3)\n",
    "\n",
    "    # create parameters\n",
    "    parameters = paddle.parameters.create(cost)\n",
    "\n",
    "    # create optimizer\n",
    "    adam_optimizer = paddle.optimizer.Adam(\n",
    "        learning_rate=2e-3,\n",
    "        regularization=paddle.optimizer.L2Regularization(rate=8e-4),\n",
    "        model_average=paddle.optimizer.ModelAverage(average_window=0.5))\n",
    "\n",
    "    # create trainer\n",
    "    trainer = paddle.trainer.SGD(\n",
    "        cost=cost, parameters=parameters, update_equation=adam_optimizer)\n",
    "\n",
    "    # End batch and end pass event handler\n",
    "    def event_handler(event):\n",
    "        if isinstance(event, paddle.event.EndIteration):\n",
    "            if event.batch_id % 100 == 0:\n",
    "                print \"\\nPass %d, Batch %d, Cost %f, %s\" % (\n",
    "                    event.pass_id, event.batch_id, event.cost, event.metrics)\n",
    "            else:\n",
    "                sys.stdout.write('.')\n",
    "                sys.stdout.flush()\n",
    "        if isinstance(event, paddle.event.EndPass):\n",
    "            with open('./params_pass_%d.tar' % event.pass_id, 'w') as f:\n",
    "                trainer.save_parameter_to_tar(f)\n",
    "\n",
    "            result = trainer.test(reader=test_reader, feeding=feeding)\n",
    "            print \"\\nTest with Pass %d, %s\" % (event.pass_id, result.metrics)\n",
    "\n",
    "    # Save the inference topology to protobuf.\n",
    "    from paddle.v2.plot import Ploter\n",
    "\n",
    "    train_title = \"Train cost\"\n",
    "    cost_ploter = Ploter(train_title)\n",
    "    step = 0\n",
    "    def event_handler_plot(event):\n",
    "        global step\n",
    "        if isinstance(event, paddle.event.EndIteration):\n",
    "            cost_ploter.append(train_title, step, event.cost)\n",
    "            cost_ploter.plot()\n",
    "            step += 1\n",
    "    \n",
    "    \n",
    "    inference_topology = paddle.topology.Topology(layers=output)\n",
    "    with open(\"./inference_topology.pkl\", 'wb') as f:\n",
    "        inference_topology.serialize_for_inference(f)\n",
    "\n",
    "    trainer.train(\n",
    "        reader=train_reader,\n",
    "        event_handler=event_handler,\n",
    "#         event_handler=event_handler_plot,\n",
    "        feeding=feeding,\n",
    "        num_passes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
