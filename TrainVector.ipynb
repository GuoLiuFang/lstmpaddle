{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import os\n",
    "import collections\n",
    "word_dict = collections.defaultdict(int)\n",
    "for dirpath, dirnames, filenames in os.walk(\"data\"):\n",
    "    print(\"dirpath---%s---dirnames---%s---filenames---%s\" % (str(dirpath), str(dirnames), str(filenames)))\n",
    "    print(\"&\"*100)\n",
    "    if len(filenames) != 0:\n",
    "        for fn in filenames:\n",
    "            print(\"file name is %s\" % fn)\n",
    "            with open(os.path.join(dirpath, fn)) as f:\n",
    "                for line in f:\n",
    "                    print(\"which line is processing ---%s---\" % line)\n",
    "                    for w in line.strip().split():\n",
    "                        print(\"which word is ---%s---\" % w)\n",
    "                        #print(\"which word is ---%s---\" % w.decode('utf-8'))\n",
    "                        word_dict[w] += 1\n",
    "                        #word_dict[w.decode('utf-8')] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key , value in word_dict.items():\n",
    "    print(\"---key---%s---value---%d\" % (key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = list(word_dict.items())\n",
    "items.sort(key=lambda x: x[1], reverse=True)\n",
    "type(items)\n",
    "for i in items:\n",
    "    print(\"===%s======%s===\" % (i[0],i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(items[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把词频给隐去了。。。。词频仅仅是为了作为排序的依据。。。所以。。len(iterms)有多长，数组就有多大。。。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict()\n",
    "for i in xrange(len(items)):\n",
    "    word_dict[items[i][0]] = i\n",
    "for key, value in word_dict.iteritems():\n",
    "    print(\"---key---%s---value---%d\" % (key,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_LIMIT =2000   # 只训练2000个词汇\n",
    "WINDOW_SIZE=11    # 训练窗口大小为11\n",
    "EMB_SIZE=32       # 设定词向量宽度\n",
    "NUM_PASSES = 20   # 设定训练轮数\n",
    "\n",
    "START_ID = WORD_LIMIT  # 句子开始标志\n",
    "END_ID = START_ID + 1  # 句子结束标志\n",
    "all_data = []\n",
    "for dirpath, dirnames, filenames in os.walk(\"data\"):\n",
    "    for fn in filenames:\n",
    "        print(\"文件名是：%s\" %fn)\n",
    "        with open(os.path.join(dirpath, fn)) as f:\n",
    "            for line in f:\n",
    "                print(\"没有加工之前---%s\" % line)\n",
    "                line = [word_dict[w] for w in line.strip().split() if word_dict[w] < WORD_LIMIT]\n",
    "                print(\"加工完了之后---%s\" % ' '.join(str(e) for e in line))\n",
    "                line = [START_ID] + line + [END_ID]\n",
    "                print(\"加上开始结束符---%s\" % ' '.join(str(e) for e in line))\n",
    "                if len(line) >= WINDOW_SIZE:\n",
    "                    all_data.append(line)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真他妈，牛逼。。。0-1999用来做词 ID。。。。2000，2001用来做句子的开始和结束。。。因为不可能重复。。。代码质量很高。。。。\n",
    "下一步，开始配置 reader_creator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def word_reader_creator():\n",
    "    def reader():\n",
    "        global all_data# 这个声明保证了all_data 可以被访问。。。\n",
    "        random.shuffle(all_data)\n",
    "        for line in all_data:\n",
    "            print(\"$$$\"*100)\n",
    "            for i in xrange(len(line) - WINDOW_SIZE + 1):\n",
    "                #从0开始，包含最后一个节点。。但是，list 是前闭后开。。。所以，正好进行\n",
    "                print(\"start---%d---end%d---content---%s\" % (i,i+WINDOW_SIZE, ' '.join(str(e) for e in line[i:i+WINDOW_SIZE])))\n",
    "                yield line[i:i+WINDOW_SIZE]\n",
    "    return reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in all_data:\n",
    "    print(\"$$$\"*100)\n",
    "    for i in xrange(len(line) - WINDOW_SIZE + 1):\n",
    "        #从0开始，包含最后一个节点。。但是，list 是前闭后开。。。所以，正好进行\n",
    "        print(\"start---%d---end%d---content---%s\" % (i,i+WINDOW_SIZE, ' '.join(str(e) for e in line[i:i+WINDOW_SIZE])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个代码实在是精妙。。。。11.。。前面是5个。。。后面是5个。。。。\n",
    "正好把中间那个给剔除了。。。\n",
    "第一：\n",
    "非常重点来了。。。。全部转化为，，等长度的11.。。。前5个。。。后5个。。。。中间1个。。。正好11个。。。。\n",
    "每个1层。。。。。。每层中的维度，最多2002种。。。。类型是词 ID。。。并不是真正的运算数据。。。\n",
    "第二：\n",
    "两个 for 循环。。。。\n",
    "第二层把每个词 ID 最终用32个属性表示。。。。\n",
    "并且把参数保存在名字叫 emb 的参数中。。。。\n",
    "接下来对这个10个层，进行 map 操作，只不过是1对1的 map 操作，并没有发生什么变化。。。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle.v2 as paddle\n",
    "paddle.init(use_gpu=False, trainer_count=3)\n",
    "words = [paddle.layer.data(name=\"word_%d\" % i, type=paddle.data_type.integer_value(WORD_LIMIT + 2)) for i in xrange(WINDOW_SIZE)]\n",
    "embs = []\n",
    "for w in words[:WINDOW_SIZE / 2 ] + words[-WINDOW_SIZE /2 + 1:]:\n",
    "    embs.append(paddle.layer.embedding(input=w, size=EMB_SIZE, param_attr=paddle.attr.Param(name=\"emb\", sparse_update=True)))\n",
    "with paddle.layer.mixed(size=EMB_SIZE) as sum_emb:\n",
    "    for emb in embs:\n",
    "        sum_emb += paddle.layer.identity_projection(input=emb)\n",
    "label = words[WINDOW_SIZE / 2]\n",
    "cost = paddle.layer.hsigmoid(input=sum_emb, label = label, num_classes=WORD_LIMIT + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = paddle.parameters.create(cost)\n",
    "optimizer = paddle.optimizer.RMSProp(learning_rate=1e-3)\n",
    "trainer = paddle.trainer.SGD(cost,parameters,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p output\n",
    "import sys\n",
    "import gzip\n",
    "total_cost = 0.0\n",
    "counter = 0\n",
    "prefix=\"./output\"\n",
    "def event_handler(event):\n",
    "    global total_cost\n",
    "    global counter\n",
    "    if isinstance(event, paddle.event.EndIteration):\n",
    "        total_cost += event.cost\n",
    "        count += 1\n",
    "        sys.stdout.write('.')\n",
    "        if event.batch_id % 100 == 0:\n",
    "            print(\"Pass %d, Batch %d, AvgCost %f\" %(event.pass_id, event.batch_id, total_cost/ counter))\n",
    "        if event.batch_id % 10000 == 0:\n",
    "            with gzip.open(os.path.join(prefix, \"model_%d_%d.tar.gz\" % (event.pass_id, event.batch_id)), 'w') as f:\n",
    "                patameters.to_tar(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    paddle.batch(\n",
    "    paddle.reader.buffered(word_reader_creator(), 16 * 4000), \n",
    "        3000),\n",
    "    num_passes=NUM_PASSES, event_handler=event_handler, feeding=[w.name for w in words])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
